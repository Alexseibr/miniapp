ПРОМПТ ДЛЯ REPLIT / CODEX — REVIEW НАГРУЗКИ, КАФКИ И ОТКАЗОУСТОЙЧИВОСТИ

Задача:
Проанализировать текущий проект KETMAR Market в этом реплите с точки зрения:

распределённой архитектуры,

использования очередей / Kafka / message broker’ов,

масштабируемости и отказоустойчивости.

Нужно понять, на каком этапе мы остановились с реализацией распределённой системы, что реально работает, а что только задумано/начато, и выдержит ли текущая архитектура нагрузку:

до 1000 одновременных пользователей онлайн,

до 2000 добавлений объявлений за короткий промежуток времени (например, в течение часа / пика нагрузки).

1. Найти и описать все элементы распределённой системы

Просканируй проект и найди всё, что связано с:

Kafka, RabbitMQ, NATS, Redis Streams или другим message broker’ом,

очередями задач (job queues) — Bull, BullMQ, Agenda, очереди в MongoDB и т.п.,

отдельными микросервисами / воркерами / consumer-сервисами.

Опиши:

какой именно брокер/очередь используется (Kafka, Redis, другое),

где он настраивается (конфиги, env-переменные),

какие топики/очереди уже существуют (названия, назначение),

какие сервисы выступают producer’ами, какие — consumer’ами.

Скажи честно:

это уже реально используемая часть системы (производство/обработка событий),

или это заготовка/черновик, который не интегрирован до конца?

2. Какие события и процессы идут через очереди

Нас интересует, что проходит через брокер/очереди и что остаётся синхронным:

Найди, какие типы событий используются:

создание объявления,

обновление объявления,

загрузка фото,

отправка SMS / уведомлений,

логирование / аналитика,

перерасчёт рейтингов, антифрод, и т.п.

Для каждого события опиши:

как оно порождается (каким сервисом/эндпоинтом),

в какой топик/очередь отправляется,

кто его потребляет (какой consumer/воркер),

что происходит в случае ошибки (retry / DLQ / логирование / игнорируется).

Ответь:

какие критичные для UX операции сейчас выполняются синхронно (в HTTP-запросе),

а какие уже вынесены в фон через очереди.

3. База данных и узкие места при нагрузке

Опиши текущее состояние работы с БД:

какая БД используется (скорее всего MongoDB, но уточни по коду),

какие основные коллекции/таблицы для:

объявлений,

пользователей,

геоданных,

логов, статистики, и т.п.

Посмотри:

какие индексы сейчас созданы (особенно по:

createdAt,

userId,

category,

гео (2dsphere),

есть ли потенциал для узких мест при вставке 2000 объявлений:

массовые вставки,

блокирующие индексы,

сложные агрегации в онлайне.

Опиши потенциальные bottleneck’и:

точки, где синхронно делается много тяжёлых операций (фильтры, сортировки, агрегации),

места, где один и тот же ресурс используется всеми (shared state, глобальные блокировки и т.п.).

4. Оценка выдержки нагрузок (1000 онлайн / 2000 объявлений)

Сделай оценку по коду: выдержит ли текущая архитектура сценарий:

до 1000 одновременных пользователей, которые:

листают ленту объявлений,

иногда создают новые объявления,

фильтруют по гео и категориям.

около 2000 новых объявлений за короткий период (пик).

Ответь:

Где сейчас самые слабые места, которые, скорее всего, не выдержат:

синхронные цепочки запросов к БД,

тяжёлые операции в основном потоке (Node.js event loop, если используется),

отсутствие очередей для тяжёлых задач (обработка фото, генерация превью, антифрод, рассылки).

Какие части системы уже хорошо подготовлены:

использование брокера/очередей,

разделение сервисов (микросервисы),

кэширование (Redis или другое),

грамотные индексы, пагинация, лимиты.

5. Отказоустойчивость и поведение при падениях

Нужно понять, что будет, если что-то упадёт.

Посмотри, есть ли:

кластеризация/scale-out backend-а (например, PM2 cluster mode, Kubernetes, docker-compose с несколькими инстансами),

механизмы reconnect'а к брокеру (Kafka/Redis) при падении,

обработка сбоев БД (retry, fallback, circuit breaker).

Для очередей / Kafka:

что происходит, если consumer падает?

сообщения теряются,

зависают,

будут обработаны повторно?

есть ли Dead Letter Queue (DLQ) или схожий механизм.

Для API:

есть ли health-check endpoints,

есть ли готовые конфиги для балансировщика (Nginx, reverse proxy, etc.),

логирование ошибок (централизованное или просто console.log).

6. На каком этапе мы остановились по распределённой системе

Сделай чёткий статус:

Что уже:

сделано и работает,

сделано частично,

только начато/задумано в виде структуры, но не интегрировано.

Отдельно опиши:

Kafka/очередь:

готова и в прод-использовании,

или пока только в коде/конфигах, но не доведена.

микросервисы/воркеры:

есть ли отдельные процессы обработки задач (фото, уведомления и т.д.),

или всё крутится в одном сервисе.

7. Рекомендации: что доработать перед боевым запуском

С учётом цели: не облажаться при первом запуске, когда сразу придёт достаточно много пользователей и объявлений.

Дай список конкретных технических рекомендаций по шагам:

что нужно доделать в первую очередь (P1), чтобы выдержать 1000 онлайн / 2000 объявлений:

докрутить очереди,

вынести тяжёлые задачи в фоновые воркеры,

добавить индексы,

ввести лимиты/ограничения.

что можно сделать во вторую очередь (P2):

улучшить логирование,

добавить кэширование,

сделать более умные retries.

что является дополнительным, но полезным (P3):

метрики (Prometheus/Grafana),

алёрты,

подробная аналитика.

Если видишь, что текущей реализации явно не хватит для вышеописанной нагрузки, напиши это прямо и предложи:

какая архитектурная схема была бы оптимальной:

backend → очередь → воркеры,

отдельный сервис для фото,

отдельный сервис для рассылок и уведомлений,

разделение read/write нагрузок и т.п.

8. Формат ответа

Пожалуйста, ответь структурировано с разделами:

Обзор: какие очереди/брокеры/распределённые части уже есть

Какие процессы проходят через очереди, а что остаётся синхронным

Состояние БД и потенциальные bottleneck’и

Оценка нагрузки (1000 онлайн / 2000 объявлений)

Отказоустойчивость и поведение при сбоях

На каком этапе остановились с распределённой системой

Конкретные рекомендации перед боевым запуском (с приоритезированным списком задач)

Кода сейчас писать не надо — только анализ, выводы и предложения по улучшению.